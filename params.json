{"name":"Cis565-finalproject","tagline":"","body":"![](img/milestone4.png)\r\n\r\nUnifided Real­time Particle Simulation Engine\r\n===============\r\n\r\n**University of Pennsylvania, CIS 565: GPU Programming and Architecture, Final Project**\r\n\r\n* [Tongbo Sui (Stonebird)](https://www.linkedin.com/in/tongbosui), [Shuai Shao (shrekshao)](https://www.linkedin.com/in/shuai-shao-3718818b)\r\n\r\n### Overview\r\nA real­time particle simulation engine implemented with CUDA. The engine includes basic particle sampling, rigid body and fluid (liquid) interaction simulations.\r\n\r\n* [slides](https://docs.google.com/presentation/d/1elaQLY8H8leIeWQ5LmkWcoFk5igO2OZbxhGEMvGc6X0/edit?usp=sharing)\r\n\r\n### Demo video\r\n[![](img/vid_demo.png)](https://youtu.be/y96n1sFAvuQ)\r\n\r\n### Installation and run\r\n* Installation\r\n\t1. Copy the entire codebase into a folder\r\n\t2. Create a new folder inside it called `build`\r\n\t3. In CMake, point \"source code\" path to your codebase, point \"build binaries\" path to `build`\r\n\t4. In CMake, click \"Configure\" and then \"Generate\". Use \"Visual Studio 12 2013\" when prompted to choose a generator\r\n\t5. Open the generated solution under `build` in Visual Studio 2013 and set StartUp project to `cis565_final`\r\n\t6. Compile a desired build\r\n* Run\r\n\t* Run with `./path-to-exe ../objs/suzanne.obj ../objs/sink.obj`\r\n\t* `objs` folder is in the top-level folder of the codebase. Please double check the relative path before running\r\n\t* `LMB`: rotate, `RMB`: translate, `Scroll`: zoom in/out\r\n\r\n### Pipeline\r\n1. Preprocessing\r\n\t1. Load objects into scene {A}\r\n\t2. Convert them to particles with particle sampling {3}{4}\r\n\t\t* Depth peeling: for each object, two depth maps are generated which maps the shallowest depth and the deepest depth respectively. Particles are filled between these two maps\r\n2. Simulation (framework based on {1})\r\n\t1. Predict particle positions based on current velocity {1}\r\n\t\t* Simple position calculation `s = v * t`\r\n\t2. Adjust particle positions based on collision tests {1}\r\n\t\t* Collision happens for each pair of particles in the local context (voxel grid) where the two particles overlap\r\n\t\t* Adjustments are made for these collisions based on how deep these overlaps are\r\n\t\t* All adjustments are averaged to produce the final adjustment value for collision\r\n\t3. Retarget fluid particles {1}{5}{7}{8}{10}\r\n\t\t* Use incompressible density as the extra constraint for the fluid particle\r\n\t\t* Poly6 Smooth Kernel is used to calculate density at each point, Spiky Smooth Kernel is used for the gradients\r\n\t\t* Calculate lambda for each fluid particle\r\n\t\t* Apply incompressibility delta to predict positions\r\n\t4. Shape matching {1}{2}\r\n\t\t* Retarget particles for rigid bodies\r\n\t\t* Matching constraint is based on the fact that rigid bodies maintain their original shapes\r\n\t\t* Based on the deformation of particles of a rigid body, it is possible to find a transformation matrix that minimizes the difference between a transformed rigid body and its deformed particle states {1}\r\n\t\t\t* Calculation is based on polar decomposition {6}{7}\r\n\t\t\t* The general method for polar decomposition is used, where matrix square root is calculated first\r\n\t\t* Such transformation matrix is used to transform the rigid body such that its particles are retargeted to maintain the shape\r\n\t5. Update final prediction and find the corresponding velocity\r\n\t\t* Adjustments due to collision and retargeting are summed and averaged to produce the final adjustments\r\n\t\t* Original prediction value is adjusted based on the final values, and velocities are calculated\r\n\t6. Update particle information based on the final values\r\n3. Render {9}\r\n\t* Particle rendering is accomplished with OpenGL built-in features\r\n\t* Render each particle as a point sprite, using the fragment shader to compute the distance to the center and get normal from texture coordinates to draw a sphere like point without using any actual geometry model\r\n\r\n### Performance\r\n* Preprocessing efficiency\r\n\t* Instead of generating two depth maps, particle sampling is performed in parallel via ray casting\r\n\t* Each ray is simply treated as z-axis unit vector, such that the ray casting is orthogonal\r\n\t* This saves time for map generation, and is equivalent to the original depth peeling algorithm\r\n* Numerical stability\r\n\t* Numerical stability plays a critical role in polar decomposition. Unstablility will cause the decomposed result to be badly conditioned, therefore causing the rotation to be unreasonably big. Such wrong result would end up with huge velocities and cause object to \"shoot\" off the scene\r\n\t* SVD method seems to be very straight-forward, and can avoid calculating matrix square roots. However, the decomposed matrices are badly conditioned\r\n\t* The more stable way is to follow the general method where a matrix square root is calculated first. However, the Jacobi method for approximating matrix square root is numerically unstable, and would cause overflows after several iterations of the entire simulation. It is also very easy for Jacobi to fail on converging to a final result\r\n\t* Denman–Beavers iteration is finally used for finding matrix square root in that it's numerically stable, and converges very fast\r\n\t* Fluid simulation now is not able to reach a stable state, numerical stability is one potential reason. \r\n* Different particle sampling resolution\r\n\t* Sampling resolution has a magnificent impact on the performance, since the complexity is `O(n^3)` for the resolution on one axis.\r\n\t* A doubled grid length will lead to a considerable improvement on the performance, meanwhile keep the simulation result at the same level.\r\n\t\r\n\t![](img/sampling_res_fps.png)\r\n\t\r\n\t|high resolution (grid length = 0.27f) | low resoultion (grid length = 0.5f) |\r\n\t|--------------------------| --------------------|\r\n\t|![](img/rigid_high_resolution.png) |![](img/rigid_low_resolution.png) |\r\n\t\r\n* Number of Rigid Bodies\r\n\t* Shape matching is done sequentially for each rigid body\r\n\t* With same number of particles, simulation slows down as number of rigid body increases\r\n\t\r\n* Global vs Voxel­based collision detection\r\n\t* For a standard test scene, the number of particles often goes up to 10,000+. It is obvious that the shorter collision test is, the better\r\n\t* For global collision tests, the complexity is stable at average case `O(n^2)` per thread. For a typical scene, this would render a loop size of 100,000,000+, which is infeasibly slow\r\n\t* One assumption can be given to the collision test that particles that are too far away must not be colliding. Therefore we can check only locally, reducing greatly the computatino complexity from polynomial to almost constant\r\n\t* Given the assumption above, particles are indexed into voxels, and for each particle during collision test, it will only test against those inside adjacent voxels. Each voxel has a fixed size, which in turn guarantees that collision test loop size will not exceed a constant\r\n\t* This effectively reduces loop size from 100,000,000+ to 150+\r\n* Time spent on different pipeline stages\r\n\t* The most time-consuming parts are collision test and fluid retargeting. Both needs to serially check surrounding particles for information updates\r\n\t* As mentioned below, it is theoretically possible to parallelize these processes to increase overall performance. Preliminary trials showed a potential of ~70% execution time reduction (3.3x speedup) when local tests are parallelized\r\n\r\n\t|Time spent on different pipeline stages |\r\n\t|--------------------------|\r\n\t|![](img/pipe_time.png) |\r\n* Fluid stablizing\r\n\t* We are now unable to achieve a stablized fluid simulation due to time limitation\r\n\t* Possible reasons we can further look into are\r\n\t\t* numerical stability\r\n\t\t* scale sensibility\r\n\t\t* successive over-relaxation parameter\r\n\r\n### Optimization\r\n* Occupancy\r\n\t* Explicitly reuse variables and optimize the computation flow to reduce # of registers used for kernels. Changing of computation flow should not add extra computations\r\n\t* Less registers gives more space to add more warps. By optimizing block size based on profiler statistics, it's possible to increase occupacy, and reduce kernel execution time\r\n\t* The part that get the most benefit is memory access, where having more warps allow more hiding of such delays\r\n\t* For simulation kernels, generally such optimization will reduce 15% execution time\r\n* Change flow to avoid repetitive computations within kernel\r\n\t* Some computations can produce expected results that can be decided upon initialization\r\n\t* Extract such computations to pre-processing so that simulation kernels don't have to spend time on them\r\n\t* Since there aren't many places for this kind of optimization, the improvement is small\r\n* Optimize memory access\r\n\t* Repetitive memory access to the same location within loops causes significant memory throttling, which is an important reason to slow kernels even with L1 caching\r\n\t* Replace such access by creating variables outside loops to cache the values to be used repetitively inside loops. This will eliminate memory throttling and reduce memory dependency by a large amount (~10%)\r\n\t* ~10% reduction on execution time\r\n* Flatten loops into separate threads, or kernel within kernel\r\n\t* For each particle, the collision test requires checking locally all particles around it. This involves huge loops inside kernel for each thread\r\n\t* It is theoretically possible to break the loops into separate threads. This would require stable concurrent updates to same memory locations\r\n\t\t* Locking: locking is extremely slow. It's not hard to imagine the reason why it's slow given the # of particles\r\n\t\t* Atomic operations: faster than locking but still super slow. After all, it involves some hardware locks that decreases performances\r\n\t* One possible way to do this is to break into two steps\r\n\t\t* The first step involves writing to a larger array, where all threads only write to its own cell\r\n\t\t* Second step is to add the results for each loop all together, producing the final (smaller) array\r\n\t\t* *However*, we haven't been able to accomplish such optimizations without rendering the result incorrect. Thus it's a possible future improvement\r\n\r\n### References\r\n* {1} Macklin, Miles, et al. \"Unified particle physics for real-time applications.\" ACM Transactions on Graphics (TOG) 33.4 (2014): 153.\r\n* {2} Müller, Matthias, et al. \"Meshless deformations based on shape matching.\" ACM Transactions on Graphics (TOG). Vol. 24. No. 3. ACM, 2005.\r\n* {3} NVIDIA CUDA Particle Tutorial [pdf](http://docs.nvidia.com/cuda/samples/5_Simulations/particles/doc/particles.pdf)\r\n* {4} Rigid body particles, GPU Gems 3 [page](http://http.developer.nvidia.com/GPUGems3/gpugems3_ch29.html)\r\n* {5} Fluid particle simulation, GPU Gems 3 [page](https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch30.html)\r\n* {6} Polar decomposition, Wikipedia [page](https://en.wikipedia.org/wiki/Polar_decomposition)\r\n* {7} Matrix square root, Wikipedia [page](https://en.wikipedia.org/wiki/Square_root_of_a_matrix)\r\n* {8} Müller, Matthias, \"Position Based Fluids.\" ACM Transactions on Graphics (TOG) 32\r\n* {9} OpenGL Point Sprite Rendering [link](http://mmmovania.blogspot.com/2011/01/point-sprites-as-spheres-in-opengl33.html)\r\n* {10} VanVerth Fluid Techniques [pdf](http://www.essentialmath.com/GDC2012/GDC2012_JMV_Fluids.pdf)\r\n\r\n### Utils\r\n* {A} Tinyobjloader [repo](https://github.com/syoyo/tinyobjloader)\r\n\r\n### Original Project Pitch\r\n#### Overview\r\n\r\nWe are going to implement a real­time particle simulation engine. The engine would proposedly\r\ninclude particle sampling, rigid body and fluid (gas or liquid) interactions. Preferably the engine would also include\r\nparticle meshing and shading via ray marching.\r\n\r\n#### Application\r\n\r\nReal­time particle simulation is useful for a wide range of purposes, especially for fields that needs\r\nsimulation demo for interactions among various bodies. For example, fluid simulation in games,\r\naerodynamics visualization, and meteorology simulation.\r\n\r\n#### Proposed Pipeline\r\n\r\nPreprocessing → Simulation → Vertex Shader (→ Geometry Shader / Meshing → Primitive Assembly\r\n→ Rasterization) → Fragment Shader\r\n\r\n* Preprocessing: load objects defined by vertices and primitives; convert them to particles with particle sampling\r\n* Simulation: physical simulation of particle interactions\r\n* Vertex shader: NDC conversion\r\n* (Geometry shader): particle meshing\r\n* (Primitive assembly): assemble meshed primitives\r\n* (Rasterization): ordinary rasterization\r\n* Fragment shader: simple ray marching for pipeline without meshing (assuming particles are uniform spheres)\r\n\r\n#### Milestone Plan\r\n\r\n* 11/16 Preprocessing, vertex shader, fragment shader (sphere ray marching)\r\n* 11/23 Simulation (solvers) Rigid Body\r\n* 11/30 Simulation (solvers) Fluid\r\n* 12/07 Simulation (solvers) / ~~Meshing~~\r\n\t\r\n#### Presentation\r\n\r\n* 11/16 Kick off, Framework setup, Rigid body sampling\r\n\t* https://docs.google.com/presentation/d/1nClJdx0G_EBylSQqkaYH11iI2RveZ4sG65v9emGyrXw/edit?usp=sharing\r\n\t\r\n\t![](img/milestone1.png)\r\n\t\r\n\t\r\n* 11/23 Milestone 2, Rigid body dynamics (some feature still buggy)\r\n\t* https://docs.google.com/presentation/d/1gCueC30LCkredTDXQ0NBadJjtKGnMahzDTzwR9Wir8Q/edit?usp=sharing\r\n\t\r\n\r\n* 11/30 Milestone 3, Rigid body dynamics fix & Fluid (buggy)\r\n\t* https://docs.google.com/presentation/d/1Q2C6C9CmGP6BEkN0zQ6VC6oX3Jrzc6MLBNzf3XELtjU/edit?usp=sharing\r\n\t\r\n\t![](img/milestone3-1.png)\r\n\t\r\n\t![](img/milestone3-3.png)\r\n\r\n* 12/7 Milestone 3, Fluid Debug (dirty) & optimization\r\n\t* https://docs.google.com/presentation/d/1Fz22_-5cNkKcdmKvkFU1XlUkZOABJLExdBBK6ca5ImA/edit?usp=sharing\r\n\t\r\n\t![](img/milestone4.png)\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}